# VLLM-Safety


## Collections of papers
- 1: Introduction
     - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 2: LVLM Architecture
     - [Natural language processing applied to mental illness detection: a narrative review](https://www.nature.com/articles/s41746-022-00589-7.)
     - [Learning transferable visual models from natural language supervision](http://proceedings.mlr.press/v139/radford21a)
     - [Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models](https://proceedings.mlr.press/v202/li23q.html)
     - [Improved baselines with visual instruction tuning](https://arxiv.org/abs/2310.03744)
     - [Visual instruction tuning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html)
     - [Minigpt-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478)
     - [Flamingo: a visual language model for few-shot learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)
     - [Minigpt-4: Enhancing vision-language understanding with advanced large language models](https://arxiv.org/abs/2304.10592)
- 3: Threaten and Attack
     - Jailbreak
        - [The title of the paper...](https://arxiv.org/abs/2403.17336)
        - Attack Include LLMs
             - [Are aligned neural networks adversarially aligned?](http://arxiv.org/abs/2306.15447)
             - [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](http://arxiv.org/abs/2311.16101)
             - [Jailbreaking Attack against Multimodal Large Language Model](http://arxiv.org/abs/2402.02309)
             - [Visual Adversarial Examples Jailbreak Aligned Large Language Models](http://arxiv.org/abs/2306.13213) 
        - Attack Exclude LLMs
             - [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](http://arxiv.org/abs/2311.05608)
             - [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](http://arxiv.org/abs/2307.14539) 
     - Backdoor
          - LLMs Backdoor Attack  
             - [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://openreview.net/pdf?id=A3y6CdiUP5)
             - [Badchain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/pdf/2401.12242.pdf))
             - [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/pdf/2310.07676.pdf)
             - [Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models](https://arxiv.org/abs/2403.17336)
             - [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/pdf/2402.13459.pdf)
             - [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://openreview.net/pdf?id=EV46z1RKhz3)
             - [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/pdf/2402.07867.pdf)
             - [Poisoning Language Models During Instruction Tuning](https://proceedings.mlr.press/v202/wan23b/wan23b.pdf)
             - [PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://arxiv.org/html/2310.12439v2)
             - [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/pdf/2402.09179.pdf)
             - [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/pdf/2402.18945.pdf)
             - [The Poison of Alignment](https://arxiv.org/pdf/2308.13449.pdf)
             - [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/pdf/2311.14455.pdf)
             - [Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](https://www.researchgate.net/profile/Shuai-Zhao-68/publication/377810700_Universal_Vulnerabilities_in_Large_Language_Models_Backdoor_Attacks_for_In-context_Learning/links/65cf68ae476dd15fb33c7a65/Universal-Vulnerabilities-in-Large-Language-Models-Backdoor-Attacks-for-In-context-Learning.pdf)
          - LVLMs Backdoor Attack
             - [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/pdf/2403.02910.pdf)
             - [Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://arxiv.org/pdf/2402.06659.pdf)
             - [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/pdf/2402.08577.pdf)
             - [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models
](https://arxiv.org/pdf/2402.13851.pdf)

     - Other Attack Methods
        - [On Evaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/abs/2305.16934.pdf)
        - [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/pdf/2402.00626.pdf)
        - [INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/pdf/2312.01886.pdf)

     - Privacy-Watermark
        - [The science of detecting llm-generated texts](https://arxiv.org/abs/2303.07205)
        - [A survey of text watermarking in the era of large language models](https://arxiv.org/abs/2312.07913)
        - Training Time Watermark
          - [Watermarking text data on large language models for dataset copyright protection](https://arxiv.org/abs/2305.13257)
          - [Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning](https://arxiv.org/abs/2402.14883)
          - [Did you train on my dataset? towards public dataset protection with cleanlabel backdoor watermarking](https://dl.acm.org/doi/abs/10.1145/3606274.3606279?casa_token=XpqF5nSd3KoAAAAA:kaqTQCvLdNFENQCzJ5JClUC1KMM7e-u9ypJ2zNJajucpuTdyXHxNTecaD098g_i6z4NBqYV8LqHYWg4)
          - [Codemark: Imperceptible watermarking for code datasets against neural code completion models](https://dl.acm.org/doi/abs/10.1145/3611643.3616297?casa_token=BRChaVDBIWAAAAAA:0ab9SSFcxWkel3nS0IJ6GlN2jLstk65GV8nzk-yHW5RyKY3wF2V9hQAURB_Plm51LhlePlYDeTQGacs)
          - [Resilient Watermarking for LLM-Generated Codes](https://arxiv.org/abs/2402.07518)
        - Generation Time Watermark
          -[A watermark for large language models](https://proceedings.mlr.press/v202/kirchenbauer23a.html)
          -[]()
          
          
        
- 4: Defense Mechanisms
     - Defense for Jailbreak
       - Datasets
            - [Red Teaming Visual Language Models](http://arxiv.org/abs/2401.12915)
            - [Self-Guard: Empower the LLM to Safeguard Itself](http://arxiv.org/abs/2310.15851)
       - Defense
            - [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](http://arxiv.org/abs/2312.10766)
            - [Diffusion Models for Adversarial Purification](http://arxiv.org/abs/2205.07460)
            - [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](http://arxiv.org/abs/2308.07308)
     - Defense for Backdoor
       - [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/pdf/2402.12026.pdf)
       - [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2402.12168.pdf)
       - [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/pdf/2308.13904.pdf)
       - [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/pdf/2312.00027.pdf)
       - [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/pdf/2311.09763.pdf)
       - [The Philosopherâ€™s Stone: Trojaning Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)
     - Defense for Other Attack Methods
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 5: Application Risk and Solution
     - Hallucination-Corresponding Solution
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
     - Misinformation
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
     - IP Infringement -> model stealing
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)


- 7: Limitations of Existing Works and Future Research Directions
    - [The title of the paper...](https://arxiv.org/abs/2403.17336)
       
