# VLLM-Safety


## Collections of papers
- 1: Introduction
     - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 2: LVLM Architecture
     - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 3: Threaten and Attack
     - Jailbreak
        - [The title of the paper...](https://arxiv.org/abs/2403.17336)
        - Attack Include LLMs
             - [Are aligned neural networks adversarially aligned?](http://arxiv.org/abs/2306.15447)
             - [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](http://arxiv.org/abs/2311.16101)
             - [Jailbreaking Attack against Multimodal Large Language Model](http://arxiv.org/abs/2402.02309)
             - [Visual Adversarial Examples Jailbreak Aligned Large Language Models](http://arxiv.org/abs/2306.13213) 
        - Attack Exclude LLMs
             - [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](http://arxiv.org/abs/2311.05608)
             - [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](http://arxiv.org/abs/2307.14539) 
     - Backdoor
          - LLMs Backdoor Attack  
             - [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://openreview.net/pdf?id=A3y6CdiUP5)
             - [Badchain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/pdf/2401.12242.pdf))
             - [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/pdf/2310.07676.pdf)
             - [Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models](https://arxiv.org/abs/2403.17336)
             - [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/pdf/2402.13459.pdf)
             - [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://openreview.net/pdf?id=EV46z1RKhz3)
             - [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/pdf/2402.07867.pdf)
             - [Poisoning Language Models During Instruction Tuning](https://proceedings.mlr.press/v202/wan23b/wan23b.pdf)
             - [PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://arxiv.org/html/2310.12439v2)
             - [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/pdf/2402.09179.pdf)
             - [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/pdf/2402.18945.pdf)
             - [The Poison of Alignment](https://arxiv.org/pdf/2308.13449.pdf)
             - [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/pdf/2311.14455.pdf)
             - [Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](https://www.researchgate.net/profile/Shuai-Zhao-68/publication/377810700_Universal_Vulnerabilities_in_Large_Language_Models_Backdoor_Attacks_for_In-context_Learning/links/65cf68ae476dd15fb33c7a65/Universal-Vulnerabilities-in-Large-Language-Models-Backdoor-Attacks-for-In-context-Learning.pdf)
          - LVLMs Backdoor Attack
             - [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/pdf/2403.02910.pdf)
             - [Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://arxiv.org/pdf/2402.06659.pdf)
             - [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/pdf/2402.08577.pdf)
             - [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models
](https://arxiv.org/pdf/2402.13851.pdf)

     - Other Attack Methods
        - [On Evaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/abs/2305.16934.pdf)
        - [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/pdf/2402.00626.pdf)
        - [INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/pdf/2312.01886.pdf)

     - Privacy-Watermark
        - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 4: Defense Mechanisms
     - Defense for Jailbreak
       - Datasets
            - [Red Teaming Visual Language Models](http://arxiv.org/abs/2401.12915)
            - [Self-Guard: Empower the LLM to Safeguard Itself](http://arxiv.org/abs/2310.15851)
       - Defense
            - [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](http://arxiv.org/abs/2312.10766)
            - [Diffusion Models for Adversarial Purification](http://arxiv.org/abs/2205.07460)
            - [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](http://arxiv.org/abs/2308.07308)
     - Defense for Backdoor
       - [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/pdf/2402.12026.pdf)
       - [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2402.12168.pdf)
       - [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/pdf/2308.13904.pdf)
       - [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/pdf/2312.00027.pdf)
       - [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/pdf/2311.09763.pdf)
       - [The Philosopherâ€™s Stone: Trojaning Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)
     - Defense for Other Attack Methods
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
- 5: Application Risk and Solution
     - Hallucination-Corresponding Solution
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
     - Misinformation
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)
     - IP Infringement -> model stealing
       - [The title of the paper...](https://arxiv.org/abs/2403.17336)


- 7: Limitations of Existing Works and Future Research Directions
    - [The title of the paper...](https://arxiv.org/abs/2403.17336)
       
